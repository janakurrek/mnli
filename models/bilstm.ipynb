{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.load_data import load_mnli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0bcd04599a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_mnli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/multi-nli/utils/load_data.py\u001b[0m in \u001b[0;36mload_mnli\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_mnli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# read data to pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmnli_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNLI_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# combine pairs and map labels to ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmnli_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnli_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"<END_OF_PAIR>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmnli_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1089\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m             )\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "load_mnli()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f59020604bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m train_data, valid_data, test_data = data.TabularDataset.splits(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dataset/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         train_data = None if train is None else cls(\n\u001b[0m\u001b[1;32m     78\u001b[0m             os.path.join(path, train), **kwargs)\n\u001b[1;32m     79\u001b[0m         val_data = None if validation is None else cls(\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmake_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmake_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/torchtext/data/example.py\u001b[0m in \u001b[0;36mfromCSV\u001b[0;34m(cls, data, fields, field_to_index)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfield_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "source = data.Field(\n",
    "        tokenize = 'spacy'\n",
    "        , lower = True\n",
    "        , batch_first = True\n",
    "        )\n",
    "\n",
    "target = data.Field(\n",
    "        sequential=False\n",
    "        , use_vocab = False\n",
    "        , is_target=True\n",
    "        )\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "    path = 'dataset/'\n",
    "    , train = 'train.csv'\n",
    "    , validation = 'val.csv'\n",
    "    , test = 'test.csv'\n",
    "    , format = 'csv'\n",
    "    , fields = {'sentence': ('text', source), 'gold_label': ('target', target)}\n",
    ")\n",
    "\n",
    "source.build_vocab(train_data, min_freq=2)\n",
    "source.vocab.load_vectors(torchtext.vocab.Vectors(glove_path, cache=\".\"))\n",
    "\n",
    "print(source.vocab.vectors.shape)\n",
    "print(f\"Unique tokens in text vocabulary: {len(source.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 4, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden = hidden.reshape((1, hidden.shape[1], hidden.shape[2] * 2))\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        catted = torch.cat([hidden, encoder_outputs], 2)\n",
    "        energy = F.relu(self.attn(catted))\n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]\n",
    "    \n",
    "class Seq2One(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embed_size,\n",
    "                 hidden_size, n_layers, dropout):\n",
    "        \n",
    "        super(Seq2One, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size,\n",
    "                            num_layers=n_layers, dropout=dropout, \n",
    "                            bidirectional=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (seq_length, N) where N is the batch size\n",
    "        embedded = self.dropout(self.embed(x.transpose(0,1)))\n",
    "        # embedded: (seq_length, N, embed_size)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        weights = self.attention(hidden[-2:], outputs)\n",
    "        \n",
    "        context = weights.bmm(outputs.transpose(0, 1))\n",
    "        context = context.transpose(0, 1)\n",
    "        \n",
    "        context = context.squeeze(0)\n",
    "        output = self.fc(context)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hyperparameters\"\"\"\n",
    "# training\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "# model hyperparameters\n",
    "load_model = False\n",
    "device = 'cpu' # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = len(source.vocab)\n",
    "output_size = len(labels)\n",
    "embed_size = 50\n",
    "hidden_size = 1024 # 2014 benchmark; slightly small\n",
    "num_layers = 2 # benchmark did 4\n",
    "dropout = 0.5\n",
    "\n",
    "# define iterator\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = batch_size,\n",
    "     sort_within_batch = True,\n",
    "     sort_key = lambda x: len(x.text), # minimize padding\n",
    "     device = device)\n",
    "\n",
    "\n",
    "model = Seq2One(input_size, output_size, embed_size, hidden_size,\n",
    "                  num_layers, dropout).to(device)\n",
    "\n",
    "\n",
    "pad_idx = source.vocab.stoi[\"<pad\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# simple cross entropy cost (might be numerically unstable if pred has 0)\n",
    "# https://discuss.pytorch.org/t/cross-entropy-with-one-hot-targets/13580/6\n",
    "def xentropy_cost(x_target, log_x_pred):\n",
    "    \"\"\"Cross Entropy for One Hot Encoded Targets\"\"\"\n",
    "    assert x_target.size() == log_x_pred.size(), \"size fail ! \" + str(x_target.size()) + \" \" + str(log_x_pred.size())\n",
    "    return -torch.sum(x_target * log_x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 20\n",
      "XEntropy Loss: tensor(140.6231, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9834, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.2542, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1848, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6089, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(144.6628, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.3262, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3078, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.9890, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(144.2816, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.6668, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3825, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8075, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.1952, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0742, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.4247, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.3235, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0714, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.8972, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.1676, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2012, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.1730, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8251, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.8952, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.4012, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3176, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6011, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.2582, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3080, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2231, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7682, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6140, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.9966, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.7528, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6194, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.5320, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.2968, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2919, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.4877, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.9022, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8750, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0236, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.2766, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6671, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5614, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.8211, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.3065, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1230, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7679, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0910, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8820, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.6754, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.9619, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.0076, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.9188, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6501, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.1583, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.5779, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.6039, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.1051, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.1168, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.9556, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.8786, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.8421, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1168, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6099, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2382, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6472, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.4607, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.6801, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2982, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.7873, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.4460, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2300, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6831, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4579, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5181, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8014, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0198, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6269, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0310, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4296, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.7592, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.7480, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2911, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8803, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.3845, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5360, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.5306, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0215, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5084, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.8535, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2494, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6938, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.3803, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2965, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8737, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6421, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3631, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.4457, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0651, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6696, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.4959, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5646, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6921, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2962, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2154, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6059, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9178, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.5435, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.9048, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(167.6449, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(144.5920, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.2586, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1055, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.7823, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2646, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5012, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2697, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6652, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.4489, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4263, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1907, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.6410, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5967, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.0401, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.3419, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5725, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2060, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.4018, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5280, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0226, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.3381, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5057, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6471, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4169, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8200, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1883, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7246, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1460, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6799, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0064, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.0791, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8410, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3183, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.7121, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2394, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1200, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4291, grad_fn=<NegBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XEntropy Loss: tensor(140.3765, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3184, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.3539, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2982, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0925, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5884, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5423, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.4417, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7161, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2836, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8852, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5652, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2905, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0388, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.6584, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6019, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2359, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8884, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.4914, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1250, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.1329, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.3389, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5682, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2559, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9294, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4944, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0656, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0594, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0955, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9967, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2882, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.8105, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2050, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9202, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1631, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.3703, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.7954, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2251, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.7660, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.7379, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0864, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.0648, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5281, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.6562, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.1329, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5577, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8468, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4099, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7988, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0454, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0076, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5642, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5781, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3530, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.7512, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.7241, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.7840, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.0683, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.3547, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5723, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1615, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2426, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.9427, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3034, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0495, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.5216, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.3672, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.9738, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.0320, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.3983, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(146.0427, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2061, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8214, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.8591, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.8120, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.8751, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.8969, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.3688, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.3021, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8142, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.1306, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6533, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8990, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.3496, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.1955, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5868, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.7876, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8975, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0407, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.4117, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5193, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2013, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5585, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.2407, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7581, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.3498, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5082, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4930, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.0361, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6904, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1609, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6579, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6324, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.1933, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8602, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.0158, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.7853, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.7927, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.2675, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.7784, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2883, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.4377, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9238, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.8918, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.5150, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.2943, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.5209, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.4594, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(134.2430, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.9535, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4006, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.2883, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(135.4768, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.0151, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.6998, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.6800, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.6536, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9268, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.2881, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.4570, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.2458, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.5602, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6732, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.1694, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.1480, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.1606, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8380, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.8410, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.4270, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.4372, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.9114, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.9121, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.4813, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1457, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1329, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.8465, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7944, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(144.0552, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.6390, grad_fn=<NegBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XEntropy Loss: tensor(140.5264, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.2075, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.7768, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.9957, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5524, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0874, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.1110, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.7516, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.6921, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.0570, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6021, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8333, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.7322, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.3794, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.1741, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.4207, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.5648, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.2680, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.1046, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.5390, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.4075, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6255, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.5275, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.7807, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9532, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.4540, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.4158, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.5566, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.7185, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.2518, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.1194, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.1092, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.0762, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.5376, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.0942, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.5268, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.7030, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.0683, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.6130, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.4416, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.5611, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.5414, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.5556, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.9371, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.7921, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7702, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.4769, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.5705, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.6124, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.0066, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7432, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.6985, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(134.9639, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.0808, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.3265, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0743, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1680, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.0703, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(134.5835, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.3964, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.8131, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.6194, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(133.9919, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.3890, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.2786, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8713, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.2919, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.1517, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9279, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.1299, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.7947, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.8159, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(135.6467, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.8126, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.6963, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.9642, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.2723, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.6609, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.0922, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.5628, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.2379, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.3662, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.9583, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.1784, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(144.1452, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.2306, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.9007, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.4632, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(134.8684, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(135.8730, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.1850, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.7292, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.3227, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.9996, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.5773, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.4260, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.2140, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.5055, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.4140, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(135.3555, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.1550, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.8721, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(134.6607, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(146.5553, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.5880, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.6288, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(144.5316, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.8547, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(142.8338, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(143.0498, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.1986, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.4275, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.0022, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.1536, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.2955, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.7578, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.2145, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.7230, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(135.1257, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.0035, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(144.3537, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.9200, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8593, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.4588, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.7655, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(139.0728, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8125, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.0170, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.8569, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.3859, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(135.7716, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(140.3456, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(141.1496, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.9552, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8855, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.8043, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.6743, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.4363, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.4169, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(135.2900, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.1427, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(136.5589, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.1341, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.1493, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(138.4025, grad_fn=<NegBackward>)\n",
      "XEntropy Loss: tensor(137.4058, grad_fn=<NegBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-852108b54291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# address gradient issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlu/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch} of {num_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        input_data = batch.text.to(device)\n",
    "        target_data = batch.target.to(device)\n",
    "        target_data_one_hot = torch.nn.functional.one_hot(target_data - 1, len(labels))\n",
    "        \n",
    "        output = model(input_data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = xentropy_cost(target_data_one_hot, output)\n",
    "        print(\"XEntropy Loss:\", loss)\n",
    "        \n",
    "        # address gradient issue\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
