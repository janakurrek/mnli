{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.load_data import GLOVE_PATH, LABEL_TO_INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contradiction': 1, 'entailment': 2, 'neutral': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL_TO_INT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:10<00:00, 37665.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74760, 50])\n",
      "Unique tokens in text vocabulary: 74760\n"
     ]
    }
   ],
   "source": [
    "source = data.Field(\n",
    "        tokenize = 'spacy'\n",
    "        , lower = True\n",
    "        , batch_first = True\n",
    ")\n",
    "\n",
    "target = data.Field(\n",
    "        sequential=False\n",
    "        , use_vocab = False\n",
    "        , is_target=True\n",
    ")\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "    path = '/home/ndg/users/jkurre/mnli/utils'\n",
    "    , train = 'train.csv'\n",
    "    , validation = 'val.csv'\n",
    "    , test = 'test.csv'\n",
    "    , format = 'csv'\n",
    "    , fields = {'sentence': ('text', source), 'gold_label': ('target', target)}\n",
    ")\n",
    "\n",
    "source.build_vocab(train_data, min_freq=2)\n",
    "source.vocab.load_vectors(torchtext.vocab.Vectors(GLOVE_PATH, cache=\".\"))\n",
    "\n",
    "print(source.vocab.vectors.shape)\n",
    "print(f\"Unique tokens in text vocabulary: {len(source.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 4, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden = hidden.reshape((1, hidden.shape[1], hidden.shape[2] * 2))\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        catted = torch.cat([hidden, encoder_outputs], 2)\n",
    "        energy = F.relu(self.attn(catted))\n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]\n",
    "    \n",
    "class Seq2One(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embed_size,\n",
    "                 hidden_size, n_layers, dropout):\n",
    "        \n",
    "        super(Seq2One, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size,\n",
    "                            num_layers=n_layers, dropout=dropout, \n",
    "                            bidirectional=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (seq_length, N) where N is the batch size\n",
    "        embedded = self.dropout(self.embed(x.transpose(0,1)))\n",
    "        # embedded: (seq_length, N, embed_size)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        weights = self.attention(hidden[-2:], outputs)\n",
    "        \n",
    "        context = weights.bmm(outputs.transpose(0, 1))\n",
    "        context = context.transpose(0, 1)\n",
    "        \n",
    "        context = context.squeeze(0)\n",
    "        output = self.fc(context)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hyperparameters\"\"\"\n",
    "# training\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# model hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = len(source.vocab)\n",
    "output_size = len(LABEL_TO_INT)\n",
    "embed_size = 50\n",
    "hidden_size = 1024 # 2014 benchmark; slightly small\n",
    "num_layers = 2 # benchmark did 4\n",
    "dropout = 0.5\n",
    "\n",
    "# define iterator\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = batch_size,\n",
    "     sort_within_batch = True,\n",
    "     sort_key = lambda x: len(x.text), # minimize padding\n",
    "     device = device)\n",
    "\n",
    "\n",
    "model = Seq2One(input_size, output_size, embed_size, hidden_size,\n",
    "                  num_layers, dropout).to(device)\n",
    "\n",
    "pad_idx = source.vocab.stoi[\"<pad\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# simple cross entropy cost (might be numerically unstable if pred has 0)\n",
    "# https://discuss.pytorch.org/t/cross-entropy-with-one-hot-targets/13580/6\n",
    "def xentropy_cost(x_target, log_x_pred):\n",
    "    \"\"\"Cross Entropy for One Hot Encoded Targets\"\"\"\n",
    "    assert x_target.size() == log_x_pred.size(), \"size fail ! \" + str(x_target.size()) + \" \" + str(log_x_pred.size())\n",
    "    return -torch.sum(x_target * log_x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 20\n",
      "XEntropy Loss: 36.02\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.57\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 36.07\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 34.86\n",
      "XEntropy Loss: 35.51\n",
      "XEntropy Loss: 35.36\n",
      "XEntropy Loss: 35.02\n",
      "XEntropy Loss: 35.09\n",
      "XEntropy Loss: 35.0\n",
      "XEntropy Loss: 34.77\n",
      "XEntropy Loss: 35.43\n",
      "XEntropy Loss: 34.89\n",
      "XEntropy Loss: 35.12\n",
      "XEntropy Loss: 35.0\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 35.4\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 34.82\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 35.06\n",
      "XEntropy Loss: 35.08\n",
      "XEntropy Loss: 35.37\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 35.08\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 35.1\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 35.21\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.23\n",
      "XEntropy Loss: 35.34\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 35.57\n",
      "XEntropy Loss: 35.15\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.42\n",
      "XEntropy Loss: 34.95\n",
      "XEntropy Loss: 35.32\n",
      "XEntropy Loss: 35.03\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.15\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 35.29\n",
      "XEntropy Loss: 34.69\n",
      "XEntropy Loss: 35.26\n",
      "XEntropy Loss: 34.97\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 36.19\n",
      "XEntropy Loss: 34.99\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.06\n",
      "XEntropy Loss: 34.95\n",
      "XEntropy Loss: 35.15\n",
      "XEntropy Loss: 35.29\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.29\n",
      "XEntropy Loss: 35.49\n",
      "XEntropy Loss: 35.75\n",
      "XEntropy Loss: 35.76\n",
      "XEntropy Loss: 35.47\n",
      "XEntropy Loss: 35.67\n",
      "XEntropy Loss: 35.24\n",
      "XEntropy Loss: 35.06\n",
      "XEntropy Loss: 35.21\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.3\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 34.91\n",
      "XEntropy Loss: 34.18\n",
      "XEntropy Loss: 36.2\n",
      "XEntropy Loss: 35.07\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.21\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 36.07\n",
      "XEntropy Loss: 35.07\n",
      "XEntropy Loss: 34.65\n",
      "XEntropy Loss: 35.6\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.49\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 36.02\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 34.93\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.38\n",
      "XEntropy Loss: 35.1\n",
      "XEntropy Loss: 35.1\n",
      "XEntropy Loss: 35.03\n",
      "XEntropy Loss: 34.82\n",
      "XEntropy Loss: 34.85\n",
      "XEntropy Loss: 35.88\n",
      "XEntropy Loss: 36.01\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.3\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 35.35\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.24\n",
      "XEntropy Loss: 35.04\n",
      "XEntropy Loss: 34.78\n",
      "XEntropy Loss: 34.82\n",
      "XEntropy Loss: 34.91\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 35.08\n",
      "XEntropy Loss: 34.51\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 34.77\n",
      "XEntropy Loss: 35.06\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.48\n",
      "XEntropy Loss: 35.26\n",
      "XEntropy Loss: 35.02\n",
      "XEntropy Loss: 34.95\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 35.08\n",
      "XEntropy Loss: 35.45\n",
      "XEntropy Loss: 35.55\n",
      "XEntropy Loss: 35.18\n",
      "XEntropy Loss: 36.36\n",
      "XEntropy Loss: 35.41\n",
      "XEntropy Loss: 35.81\n",
      "XEntropy Loss: 35.78\n",
      "XEntropy Loss: 34.85\n",
      "XEntropy Loss: 35.28\n",
      "XEntropy Loss: 35.4\n",
      "XEntropy Loss: 35.21\n",
      "XEntropy Loss: 35.5\n",
      "XEntropy Loss: 34.78\n",
      "XEntropy Loss: 35.15\n",
      "XEntropy Loss: 35.43\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.42\n",
      "XEntropy Loss: 35.09\n",
      "XEntropy Loss: 34.64\n",
      "XEntropy Loss: 35.47\n",
      "XEntropy Loss: 35.47\n",
      "XEntropy Loss: 35.45\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.36\n",
      "XEntropy Loss: 34.63\n",
      "XEntropy Loss: 35.64\n",
      "XEntropy Loss: 34.69\n",
      "XEntropy Loss: 35.96\n",
      "XEntropy Loss: 34.55\n",
      "XEntropy Loss: 35.59\n",
      "XEntropy Loss: 35.0\n",
      "XEntropy Loss: 35.08\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 34.62\n",
      "XEntropy Loss: 34.71\n",
      "XEntropy Loss: 36.12\n",
      "XEntropy Loss: 36.32\n",
      "XEntropy Loss: 35.63\n",
      "XEntropy Loss: 34.68\n",
      "XEntropy Loss: 35.51\n",
      "XEntropy Loss: 35.0\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 35.89\n",
      "XEntropy Loss: 35.75\n",
      "XEntropy Loss: 35.23\n",
      "XEntropy Loss: 35.67\n",
      "XEntropy Loss: 34.97\n",
      "XEntropy Loss: 35.08\n",
      "XEntropy Loss: 35.54\n",
      "XEntropy Loss: 34.33\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 34.81\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.4\n",
      "XEntropy Loss: 35.01\n",
      "XEntropy Loss: 35.39\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.64\n",
      "XEntropy Loss: 35.36\n",
      "XEntropy Loss: 35.38\n",
      "XEntropy Loss: 34.84\n",
      "XEntropy Loss: 36.33\n",
      "XEntropy Loss: 35.26\n",
      "XEntropy Loss: 36.32\n",
      "XEntropy Loss: 35.38\n",
      "XEntropy Loss: 35.41\n",
      "XEntropy Loss: 34.91\n",
      "XEntropy Loss: 35.29\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.62\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.41\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 35.39\n",
      "XEntropy Loss: 34.74\n",
      "XEntropy Loss: 35.3\n",
      "XEntropy Loss: 35.72\n",
      "XEntropy Loss: 35.21\n",
      "XEntropy Loss: 35.15\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 35.91\n",
      "XEntropy Loss: 36.2\n",
      "XEntropy Loss: 35.12\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 34.9\n",
      "XEntropy Loss: 35.12\n",
      "XEntropy Loss: 35.15\n",
      "XEntropy Loss: 35.31\n",
      "XEntropy Loss: 34.86\n",
      "XEntropy Loss: 35.64\n",
      "XEntropy Loss: 35.07\n",
      "XEntropy Loss: 35.58\n",
      "XEntropy Loss: 35.09\n",
      "XEntropy Loss: 35.8\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 35.37\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 34.85\n",
      "XEntropy Loss: 34.89\n",
      "XEntropy Loss: 35.46\n",
      "XEntropy Loss: 34.95\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.42\n",
      "XEntropy Loss: 35.01\n",
      "XEntropy Loss: 34.91\n",
      "XEntropy Loss: 34.72\n",
      "XEntropy Loss: 35.02\n",
      "XEntropy Loss: 34.99\n",
      "XEntropy Loss: 35.12\n",
      "XEntropy Loss: 34.89\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 34.86\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.23\n",
      "XEntropy Loss: 35.52\n",
      "XEntropy Loss: 34.79\n",
      "XEntropy Loss: 34.4\n",
      "XEntropy Loss: 35.93\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 34.27\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 35.95\n",
      "XEntropy Loss: 35.59\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.1\n",
      "XEntropy Loss: 34.93\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.72\n",
      "XEntropy Loss: 35.42\n",
      "XEntropy Loss: 34.97\n",
      "XEntropy Loss: 34.61\n",
      "XEntropy Loss: 35.4\n",
      "XEntropy Loss: 35.34\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 35.26\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 34.52\n",
      "XEntropy Loss: 35.01\n",
      "XEntropy Loss: 34.01\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 36.15\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 35.49\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 34.89\n",
      "XEntropy Loss: 35.0\n",
      "XEntropy Loss: 35.73\n",
      "XEntropy Loss: 35.32\n",
      "XEntropy Loss: 35.6\n",
      "XEntropy Loss: 35.41\n",
      "XEntropy Loss: 34.91\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 35.12\n",
      "XEntropy Loss: 35.55\n",
      "XEntropy Loss: 35.03\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 35.4\n",
      "XEntropy Loss: 35.43\n",
      "XEntropy Loss: 35.81\n",
      "XEntropy Loss: 36.47\n",
      "XEntropy Loss: 36.41\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 35.09\n",
      "XEntropy Loss: 35.23\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.5\n",
      "XEntropy Loss: 36.21\n",
      "XEntropy Loss: 34.5\n",
      "XEntropy Loss: 35.01\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 35.96\n",
      "XEntropy Loss: 34.57\n",
      "XEntropy Loss: 35.24\n",
      "XEntropy Loss: 34.97\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 36.56\n",
      "XEntropy Loss: 34.99\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 35.97\n",
      "XEntropy Loss: 35.21\n",
      "XEntropy Loss: 35.15\n",
      "XEntropy Loss: 35.42\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.06\n",
      "XEntropy Loss: 35.31\n",
      "XEntropy Loss: 35.65\n",
      "XEntropy Loss: 35.26\n",
      "XEntropy Loss: 35.77\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 34.61\n",
      "XEntropy Loss: 34.99\n",
      "XEntropy Loss: 35.0\n",
      "XEntropy Loss: 35.35\n",
      "XEntropy Loss: 34.93\n",
      "XEntropy Loss: 35.82\n",
      "XEntropy Loss: 35.44\n",
      "XEntropy Loss: 35.83\n",
      "XEntropy Loss: 35.29\n",
      "XEntropy Loss: 35.44\n",
      "XEntropy Loss: 35.0\n",
      "XEntropy Loss: 35.9\n",
      "XEntropy Loss: 35.04\n",
      "XEntropy Loss: 34.94\n",
      "XEntropy Loss: 34.13\n",
      "XEntropy Loss: 35.46\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 34.75\n",
      "XEntropy Loss: 35.18\n",
      "XEntropy Loss: 35.75\n",
      "XEntropy Loss: 35.02\n",
      "XEntropy Loss: 34.82\n",
      "XEntropy Loss: 35.88\n",
      "XEntropy Loss: 34.13\n",
      "XEntropy Loss: 36.93\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.92\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.81\n",
      "XEntropy Loss: 36.48\n",
      "XEntropy Loss: 35.77\n",
      "XEntropy Loss: 35.24\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.09\n",
      "XEntropy Loss: 34.87\n",
      "XEntropy Loss: 34.76\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.56\n",
      "XEntropy Loss: 35.04\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.47\n",
      "XEntropy Loss: 35.86\n",
      "XEntropy Loss: 36.4\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 35.19\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.23\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 34.52\n",
      "XEntropy Loss: 35.08\n",
      "XEntropy Loss: 36.87\n",
      "XEntropy Loss: 35.09\n",
      "XEntropy Loss: 34.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XEntropy Loss: 36.66\n",
      "XEntropy Loss: 35.07\n",
      "XEntropy Loss: 34.08\n",
      "XEntropy Loss: 35.34\n",
      "XEntropy Loss: 35.18\n",
      "XEntropy Loss: 35.41\n",
      "XEntropy Loss: 35.1\n",
      "XEntropy Loss: 34.34\n",
      "XEntropy Loss: 34.93\n",
      "XEntropy Loss: 35.38\n",
      "XEntropy Loss: 35.82\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 33.59\n",
      "XEntropy Loss: 34.97\n",
      "XEntropy Loss: 34.86\n",
      "XEntropy Loss: 33.98\n",
      "XEntropy Loss: 35.41\n",
      "XEntropy Loss: 35.61\n",
      "XEntropy Loss: 35.56\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.02\n",
      "XEntropy Loss: 35.78\n",
      "XEntropy Loss: 36.2\n",
      "XEntropy Loss: 34.85\n",
      "XEntropy Loss: 37.72\n",
      "XEntropy Loss: 35.6\n",
      "XEntropy Loss: 35.35\n",
      "XEntropy Loss: 35.56\n",
      "XEntropy Loss: 35.83\n",
      "XEntropy Loss: 35.03\n",
      "XEntropy Loss: 35.87\n",
      "XEntropy Loss: 35.67\n",
      "XEntropy Loss: 35.24\n",
      "XEntropy Loss: 35.92\n",
      "XEntropy Loss: 34.95\n",
      "XEntropy Loss: 34.63\n",
      "XEntropy Loss: 34.93\n",
      "XEntropy Loss: 35.77\n",
      "XEntropy Loss: 34.62\n",
      "XEntropy Loss: 35.44\n",
      "XEntropy Loss: 35.0\n",
      "XEntropy Loss: 35.21\n",
      "XEntropy Loss: 35.94\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 35.06\n",
      "XEntropy Loss: 34.78\n",
      "XEntropy Loss: 34.78\n",
      "XEntropy Loss: 34.51\n",
      "XEntropy Loss: 36.25\n",
      "XEntropy Loss: 35.87\n",
      "XEntropy Loss: 34.35\n",
      "XEntropy Loss: 35.67\n",
      "XEntropy Loss: 35.28\n",
      "XEntropy Loss: 36.48\n",
      "XEntropy Loss: 35.85\n",
      "XEntropy Loss: 36.16\n",
      "XEntropy Loss: 36.07\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 36.31\n",
      "XEntropy Loss: 35.03\n",
      "XEntropy Loss: 35.06\n",
      "XEntropy Loss: 34.55\n",
      "XEntropy Loss: 35.69\n",
      "XEntropy Loss: 36.38\n",
      "XEntropy Loss: 36.48\n",
      "XEntropy Loss: 34.65\n",
      "XEntropy Loss: 34.48\n",
      "XEntropy Loss: 35.37\n",
      "XEntropy Loss: 49.2\n",
      "XEntropy Loss: 33.92\n",
      "XEntropy Loss: 36.01\n",
      "XEntropy Loss: 37.2\n",
      "XEntropy Loss: 35.35\n",
      "XEntropy Loss: 37.8\n",
      "XEntropy Loss: 35.58\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 37.61\n",
      "XEntropy Loss: 35.72\n",
      "XEntropy Loss: 34.93\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.53\n",
      "XEntropy Loss: 35.62\n",
      "XEntropy Loss: 36.52\n",
      "XEntropy Loss: 36.01\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 35.8\n",
      "XEntropy Loss: 36.93\n",
      "XEntropy Loss: 35.45\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 34.32\n",
      "XEntropy Loss: 34.5\n",
      "XEntropy Loss: 35.98\n",
      "XEntropy Loss: 35.65\n",
      "XEntropy Loss: 37.47\n",
      "XEntropy Loss: 34.02\n",
      "XEntropy Loss: 34.12\n",
      "XEntropy Loss: 35.08\n",
      "XEntropy Loss: 37.32\n",
      "XEntropy Loss: 34.91\n",
      "XEntropy Loss: 37.75\n",
      "XEntropy Loss: 33.97\n",
      "XEntropy Loss: 36.88\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 34.49\n",
      "XEntropy Loss: 35.42\n",
      "XEntropy Loss: 34.67\n",
      "XEntropy Loss: 36.84\n",
      "XEntropy Loss: 34.28\n",
      "XEntropy Loss: 35.87\n",
      "XEntropy Loss: 34.86\n",
      "XEntropy Loss: 35.66\n",
      "XEntropy Loss: 34.95\n",
      "XEntropy Loss: 36.62\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 34.49\n",
      "XEntropy Loss: 35.16\n",
      "XEntropy Loss: 35.67\n",
      "XEntropy Loss: 34.57\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 34.65\n",
      "XEntropy Loss: 34.97\n",
      "XEntropy Loss: 34.77\n",
      "XEntropy Loss: 35.9\n",
      "XEntropy Loss: 35.15\n",
      "XEntropy Loss: 35.12\n",
      "XEntropy Loss: 35.57\n",
      "XEntropy Loss: 34.97\n",
      "XEntropy Loss: 36.03\n",
      "XEntropy Loss: 35.63\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 36.15\n",
      "XEntropy Loss: 34.66\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 35.33\n",
      "XEntropy Loss: 35.47\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.76\n",
      "XEntropy Loss: 35.04\n",
      "XEntropy Loss: 37.05\n",
      "XEntropy Loss: 35.09\n",
      "XEntropy Loss: 35.18\n",
      "XEntropy Loss: 35.47\n",
      "XEntropy Loss: 35.08\n",
      "XEntropy Loss: 35.47\n",
      "XEntropy Loss: 35.38\n",
      "XEntropy Loss: 34.64\n",
      "XEntropy Loss: 36.34\n",
      "XEntropy Loss: 34.63\n",
      "XEntropy Loss: 36.4\n",
      "XEntropy Loss: 35.76\n",
      "XEntropy Loss: 34.57\n",
      "XEntropy Loss: 35.54\n",
      "XEntropy Loss: 34.93\n",
      "XEntropy Loss: 34.14\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 36.11\n",
      "XEntropy Loss: 34.47\n",
      "XEntropy Loss: 35.41\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.56\n",
      "XEntropy Loss: 34.95\n",
      "XEntropy Loss: 35.44\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 36.19\n",
      "XEntropy Loss: 35.34\n",
      "XEntropy Loss: 35.35\n",
      "XEntropy Loss: 35.62\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.12\n",
      "XEntropy Loss: 35.01\n",
      "XEntropy Loss: 35.73\n",
      "XEntropy Loss: 35.03\n",
      "XEntropy Loss: 35.64\n",
      "XEntropy Loss: 33.99\n",
      "XEntropy Loss: 35.52\n",
      "XEntropy Loss: 35.7\n",
      "XEntropy Loss: 35.52\n",
      "XEntropy Loss: 36.32\n",
      "XEntropy Loss: 34.82\n",
      "XEntropy Loss: 34.75\n",
      "XEntropy Loss: 35.01\n",
      "XEntropy Loss: 34.83\n",
      "XEntropy Loss: 35.28\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 35.81\n",
      "XEntropy Loss: 35.64\n",
      "XEntropy Loss: 35.33\n",
      "XEntropy Loss: 35.5\n",
      "XEntropy Loss: 34.29\n",
      "XEntropy Loss: 35.42\n",
      "XEntropy Loss: 35.48\n",
      "XEntropy Loss: 35.4\n",
      "XEntropy Loss: 34.65\n",
      "XEntropy Loss: 35.47\n",
      "XEntropy Loss: 35.52\n",
      "XEntropy Loss: 35.42\n",
      "XEntropy Loss: 36.29\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 34.36\n",
      "XEntropy Loss: 36.61\n",
      "XEntropy Loss: 35.02\n",
      "XEntropy Loss: 34.48\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.21\n",
      "XEntropy Loss: 35.63\n",
      "XEntropy Loss: 35.02\n",
      "XEntropy Loss: 35.95\n",
      "XEntropy Loss: 35.64\n",
      "XEntropy Loss: 33.24\n",
      "XEntropy Loss: 35.95\n",
      "XEntropy Loss: 35.31\n",
      "XEntropy Loss: 35.01\n",
      "XEntropy Loss: 35.37\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 35.04\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 35.04\n",
      "XEntropy Loss: 35.71\n",
      "XEntropy Loss: 34.9\n",
      "XEntropy Loss: 35.06\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 35.7\n",
      "XEntropy Loss: 35.29\n",
      "XEntropy Loss: 35.13\n",
      "XEntropy Loss: 35.28\n",
      "XEntropy Loss: 35.43\n",
      "XEntropy Loss: 34.84\n",
      "XEntropy Loss: 35.49\n",
      "XEntropy Loss: 35.04\n",
      "XEntropy Loss: 34.76\n",
      "XEntropy Loss: 34.42\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 37.02\n",
      "XEntropy Loss: 35.31\n",
      "XEntropy Loss: 34.58\n",
      "XEntropy Loss: 35.76\n",
      "XEntropy Loss: 35.04\n",
      "XEntropy Loss: 35.57\n",
      "XEntropy Loss: 34.89\n",
      "XEntropy Loss: 35.34\n",
      "XEntropy Loss: 35.56\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 35.38\n",
      "XEntropy Loss: 35.51\n",
      "XEntropy Loss: 35.26\n",
      "XEntropy Loss: 35.26\n",
      "XEntropy Loss: 35.21\n",
      "XEntropy Loss: 35.01\n",
      "XEntropy Loss: 35.46\n",
      "XEntropy Loss: 34.8\n",
      "XEntropy Loss: 35.8\n",
      "XEntropy Loss: 34.65\n",
      "XEntropy Loss: 35.36\n",
      "XEntropy Loss: 35.88\n",
      "XEntropy Loss: 35.42\n",
      "XEntropy Loss: 35.58\n",
      "XEntropy Loss: 35.47\n",
      "XEntropy Loss: 34.88\n",
      "XEntropy Loss: 36.77\n",
      "XEntropy Loss: 35.39\n",
      "XEntropy Loss: 34.87\n",
      "XEntropy Loss: 35.4\n",
      "XEntropy Loss: 34.82\n",
      "XEntropy Loss: 34.88\n",
      "XEntropy Loss: 35.77\n",
      "XEntropy Loss: 35.22\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 33.69\n",
      "XEntropy Loss: 37.5\n",
      "XEntropy Loss: 36.09\n",
      "XEntropy Loss: 34.59\n",
      "XEntropy Loss: 35.05\n",
      "XEntropy Loss: 34.92\n",
      "XEntropy Loss: 35.62\n",
      "XEntropy Loss: 34.87\n",
      "XEntropy Loss: 34.77\n",
      "XEntropy Loss: 34.16\n",
      "XEntropy Loss: 34.83\n",
      "XEntropy Loss: 34.91\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.07\n",
      "XEntropy Loss: 35.11\n",
      "XEntropy Loss: 35.07\n",
      "XEntropy Loss: 34.87\n",
      "XEntropy Loss: 35.37\n",
      "XEntropy Loss: 35.34\n",
      "XEntropy Loss: 34.8\n",
      "XEntropy Loss: 35.18\n",
      "XEntropy Loss: 35.07\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 34.34\n",
      "XEntropy Loss: 36.11\n",
      "XEntropy Loss: 35.31\n",
      "XEntropy Loss: 36.17\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 35.4\n",
      "XEntropy Loss: 35.15\n",
      "XEntropy Loss: 35.49\n",
      "XEntropy Loss: 35.02\n",
      "XEntropy Loss: 35.27\n",
      "XEntropy Loss: 35.91\n",
      "XEntropy Loss: 34.7\n",
      "XEntropy Loss: 35.52\n",
      "XEntropy Loss: 34.85\n",
      "XEntropy Loss: 33.95\n",
      "XEntropy Loss: 35.48\n",
      "XEntropy Loss: 35.32\n",
      "XEntropy Loss: 35.03\n",
      "XEntropy Loss: 34.84\n",
      "XEntropy Loss: 36.2\n",
      "XEntropy Loss: 35.37\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 34.62\n",
      "XEntropy Loss: 35.49\n",
      "XEntropy Loss: 35.75\n",
      "XEntropy Loss: 35.72\n",
      "XEntropy Loss: 35.12\n",
      "XEntropy Loss: 34.88\n",
      "XEntropy Loss: 34.86\n",
      "XEntropy Loss: 34.66\n",
      "XEntropy Loss: 35.33\n",
      "XEntropy Loss: 34.67\n",
      "XEntropy Loss: 34.3\n",
      "XEntropy Loss: 34.98\n",
      "XEntropy Loss: 35.6\n",
      "XEntropy Loss: 35.66\n",
      "XEntropy Loss: 34.99\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 35.06\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 35.59\n",
      "XEntropy Loss: 34.83\n",
      "XEntropy Loss: 34.91\n",
      "XEntropy Loss: 35.81\n",
      "XEntropy Loss: 35.26\n",
      "XEntropy Loss: 34.21\n",
      "XEntropy Loss: 5.53\n",
      "XEntropy Loss: 35.82\n",
      "XEntropy Loss: 34.96\n",
      "XEntropy Loss: 34.62\n",
      "Epoch 1 of 20\n",
      "XEntropy Loss: 35.23\n",
      "XEntropy Loss: 35.89\n",
      "XEntropy Loss: 36.1\n",
      "XEntropy Loss: 35.18\n",
      "XEntropy Loss: 35.55\n",
      "XEntropy Loss: 36.27\n",
      "XEntropy Loss: 35.39\n",
      "XEntropy Loss: 34.16\n",
      "XEntropy Loss: 37.38\n",
      "XEntropy Loss: 36.17\n",
      "XEntropy Loss: 35.78\n",
      "XEntropy Loss: 34.56\n",
      "XEntropy Loss: 36.49\n",
      "XEntropy Loss: 35.71\n",
      "XEntropy Loss: 35.32\n",
      "XEntropy Loss: 35.58\n",
      "XEntropy Loss: 35.87\n",
      "XEntropy Loss: 36.38\n",
      "XEntropy Loss: 35.14\n",
      "XEntropy Loss: 34.78\n",
      "XEntropy Loss: 33.93\n",
      "XEntropy Loss: 35.46\n",
      "XEntropy Loss: 34.55\n",
      "XEntropy Loss: 35.9\n",
      "XEntropy Loss: 34.99\n",
      "XEntropy Loss: 36.55\n",
      "XEntropy Loss: 35.35\n",
      "XEntropy Loss: 35.07\n",
      "XEntropy Loss: 36.08\n",
      "XEntropy Loss: 35.34\n",
      "XEntropy Loss: 34.69\n",
      "XEntropy Loss: 35.82\n",
      "XEntropy Loss: 35.1\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 34.58\n",
      "XEntropy Loss: 35.2\n",
      "XEntropy Loss: 35.18\n",
      "XEntropy Loss: 35.01\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.85\n",
      "XEntropy Loss: 35.07\n",
      "XEntropy Loss: 35.17\n",
      "XEntropy Loss: 35.51\n",
      "XEntropy Loss: 34.05\n",
      "XEntropy Loss: 36.48\n",
      "XEntropy Loss: 34.83\n",
      "XEntropy Loss: 34.61\n",
      "XEntropy Loss: 36.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XEntropy Loss: 35.32\n",
      "XEntropy Loss: 34.84\n",
      "XEntropy Loss: 35.45\n",
      "XEntropy Loss: 33.58\n",
      "XEntropy Loss: 34.29\n",
      "XEntropy Loss: 36.71\n",
      "XEntropy Loss: 34.81\n",
      "XEntropy Loss: 35.72\n",
      "XEntropy Loss: 35.25\n",
      "XEntropy Loss: 35.56\n",
      "XEntropy Loss: 35.18\n",
      "XEntropy Loss: 34.56\n",
      "XEntropy Loss: 34.99\n",
      "XEntropy Loss: 35.46\n",
      "XEntropy Loss: 36.13\n",
      "XEntropy Loss: 35.9\n",
      "XEntropy Loss: 34.92\n",
      "XEntropy Loss: 34.51\n",
      "XEntropy Loss: 35.26\n",
      "XEntropy Loss: 35.34\n",
      "XEntropy Loss: 36.05\n",
      "XEntropy Loss: 34.13\n",
      "XEntropy Loss: 35.66\n",
      "XEntropy Loss: 34.34\n",
      "XEntropy Loss: 36.82\n",
      "XEntropy Loss: 36.23\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f\"Epoch {epoch} of {num_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        input_data = batch.text.to(device)\n",
    "        target_data = batch.target.to(device)\n",
    "        target_data_one_hot = torch.nn.functional.one_hot(target_data - 1, len(LABEL_TO_INT))\n",
    "        \n",
    "        output = model(input_data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = xentropy_cost(target_data_one_hot, output)\n",
    "        \n",
    "        if batch_idx % 10 == 9:\n",
    "            print(\"XEntropy Loss:\", round(loss.item(),2))\n",
    "        \n",
    "        # address gradient issue\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss =+ loss.item() * batch_size\n",
    "    \n",
    "    loss_values.append(running_loss)\n",
    "\n",
    "plt.plot(loss_values)  \n",
    "\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "print(\"Model weight:\")    \n",
    "print(model.weight)\n",
    "\n",
    "print(\"Model bias:\")    \n",
    "print(model.bias)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "    \n",
    "PATH = '/home/ndg/users/jkurre/mnli/models/bilstm.pt'\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
