{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as O\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, vocab, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self):\n",
    "        # gpu\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # word vectors\n",
    "        self.embed_size = 50\n",
    "        self.word_vectors = True\n",
    "        self.glove_path = '/home/ndg/users/jkurre/mnli/utils/embeddings/glove.6B.50d.txt'\n",
    "        # model configs\n",
    "        self.hidden_size = 1024\n",
    "        self.batch_size = 32\n",
    "        self.input_size = 76790\n",
    "        self.output_size = 4\n",
    "        self.n_layers = 2\n",
    "        self.n_cells = 4\n",
    "        self.dropout = 0.5\n",
    "        # training\n",
    "        self.epochs = 5\n",
    "        self.learning_rate = 0.0001\n",
    "        self.outpath = '/home/ndg/users/jkurre/mnli/models/bilstm_revised.pt' # _onehot.pt\n",
    "        self.outfile = '/outputs/bilstm_with_attention.txt'\n",
    "\n",
    "params = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data.Field(\n",
    "    lower=True,\n",
    "    tokenize='spacy'\n",
    ")\n",
    "\n",
    "answers = data.Field(\n",
    "    sequential=False\n",
    ")\n",
    "\n",
    "train, val, test = datasets.MultiNLI.splits(\n",
    "    text_field=inputs,\n",
    "    label_field=answers\n",
    "    )\n",
    "\n",
    "inputs.build_vocab(train, val, test)\n",
    "\n",
    "if params.word_vectors:\n",
    "    inputs.vocab.load_vectors(vocab.Vectors(params.glove_path, cache=\".\"))\n",
    "\n",
    "answers.build_vocab(train)\n",
    "\n",
    "params.n_embed = len(inputs.vocab)\n",
    "params.d_out = len(answers.vocab)\n",
    "\n",
    "print(f\"Unique tokens in inputs vocabulary: {params.n_embed}\")\n",
    "print(f\"Unique tokens in answers vocabulary: {params.d_out}\")\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=params.batch_size, device=params.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 4, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden = hidden.reshape((1, hidden.shape[1], hidden.shape[2] * 2))\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        catted = torch.cat([hidden, encoder_outputs], 2)\n",
    "        energy = F.relu(self.attn(catted))\n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]\n",
    "    \n",
    "class MultiNLIModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embed_size, device,\n",
    "                 hidden_size, batch_size, dropout, n_layers, n_cells):\n",
    "        \n",
    "        super(MultiNLIModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_cells = n_cells\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size,\n",
    "                            num_layers=n_layers, dropout=dropout, \n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n",
    "        self.fc_output = nn.Linear(hidden_size,  output_size, bias=False)\n",
    "    \n",
    "    def encode(self, embed):\n",
    "        # pass embedding input through lstm\n",
    "        state_shape = self.n_cells, self.batch_size, self.hidden_size\n",
    "        h0 = c0 = embed.new_zeros(state_shape)\n",
    "        outputs, (ht, ct) = self.lstm(embed, (h0, c0))\n",
    "\n",
    "        # pass outcomes through attention layer\n",
    "        weights = self.attention(ht[-2:], outputs)\n",
    "        context = weights.bmm(outputs.transpose(0, 1))\n",
    "        context = context.transpose(0, 1)\n",
    "        context = context.squeeze(0)\n",
    "        return context\n",
    "        \n",
    "    def forward(self, pair):\n",
    "        \n",
    "        # seq_length, batch_size, embed_size\n",
    "        prem_embed = self.dropout(self.embed(pair.premise))\n",
    "        hypo_embed = self.dropout(self.embed(pair.hypothesis))\n",
    "        \n",
    "        prem_contx = self.encode(prem_embed)\n",
    "        hypo_contx = self.encode(hypo_embed)\n",
    "        \n",
    "        # seq_len, hidden_size * 2\n",
    "        pair_embed = prem_contx - hypo_contx\n",
    "        pair_embed = self.relu(self.fc_hidden(pair_embed))\n",
    "        \n",
    "        # hidden_size * 2, output_size\n",
    "        pair_output = self.relu(self.fc_output(pair_embed))\n",
    "        \n",
    "        return pair_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiNLIModel(params.input_size, params.output_size, params.embed_size, params.device,\n",
    "                      params.hidden_size, params.batch_size, params.dropout, params.n_layers, params.n_cells).to(params.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 7.93 GiB total capacity; 2.24 GiB already allocated; 19.06 MiB free; 2.46 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f24a15e4536 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x7f24a182df1e in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x7f24a182ef9e in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x135 (0x7f24533cd9e5 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xf688bb (0x7f24519b98bb in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xfb21a7 (0x7f2451a031a7 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0x1073c49 (0x7f248e2d7c49 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x1073f87 (0x7f248e2d7f87 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xf1f1ae (0x7f24519701ae in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: <unknown function> + 0xf2622f (0x7f245197722f in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #10: at::native::_cudnn_rnn_backward(at::Tensor const&, c10::ArrayRef<at::Tensor>, long, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long, long, long, bool, double, bool, bool, c10::ArrayRef<long>, at::Tensor const&, at::Tensor const&, std::array<bool, 4ul>) + 0x2c8 (0x7f245197e688 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: <unknown function> + 0xfb02fd (0x7f2451a012fd in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xfb1a93 (0x7f2451a02a93 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #13: <unknown function> + 0x2b081d0 (0x7f248fd6c1d0 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x2b7b623 (0x7f248fddf623 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: torch::autograd::generated::CudnnRnnBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x708 (0x7f248fb20d18 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: <unknown function> + 0x2d89705 (0x7f248ffed705 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f248ffeaa03 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f248ffeb7e2 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f248ffe3e59 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f24a236b5f8 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #21: <unknown function> + 0xbd6df (0x7f24be5426df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #22: <unknown function> + 0x76db (0x7f24c38d16db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #23: clone + 0x3f (0x7f24c3c0aa3f in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9690b0d42f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 7.93 GiB total capacity; 2.24 GiB already allocated; 19.06 MiB free; 2.46 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f24a15e4536 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x7f24a182df1e in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x7f24a182ef9e in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x135 (0x7f24533cd9e5 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xf688bb (0x7f24519b98bb in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xfb21a7 (0x7f2451a031a7 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0x1073c49 (0x7f248e2d7c49 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x1073f87 (0x7f248e2d7f87 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xf1f1ae (0x7f24519701ae in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: <unknown function> + 0xf2622f (0x7f245197722f in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #10: at::native::_cudnn_rnn_backward(at::Tensor const&, c10::ArrayRef<at::Tensor>, long, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long, long, long, bool, double, bool, bool, c10::ArrayRef<long>, at::Tensor const&, at::Tensor const&, std::array<bool, 4ul>) + 0x2c8 (0x7f245197e688 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: <unknown function> + 0xfb02fd (0x7f2451a012fd in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xfb1a93 (0x7f2451a02a93 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #13: <unknown function> + 0x2b081d0 (0x7f248fd6c1d0 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x2b7b623 (0x7f248fddf623 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: torch::autograd::generated::CudnnRnnBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x708 (0x7f248fb20d18 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: <unknown function> + 0x2d89705 (0x7f248ffed705 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f248ffeaa03 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f248ffeb7e2 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f248ffe3e59 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f24a236b5f8 in /home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #21: <unknown function> + 0xbd6df (0x7f24be5426df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #22: <unknown function> + 0x76db (0x7f24c38d16db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #23: clone + 0x3f (0x7f24c3c0aa3f in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = O.Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "val_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{}'.split(','))\n",
    "log_template =  ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\n",
    "\n",
    "iterations = 0\n",
    "start = time.time()\n",
    "\n",
    "acc_loss = []\n",
    "\n",
    "for epoch in range(params.epochs):\n",
    "    train_iterator.init_epoch()\n",
    "    n_correct, n_total = 0, 0\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        \n",
    "        # switch model to training mode, clear gradient accumulators\n",
    "        model.train();\n",
    "        opt.zero_grad()\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "        # forward pass\n",
    "        answer = model(batch)\n",
    "        \n",
    "        # calculate accuracy of predictions in the current batch\n",
    "        n_correct += (torch.max(answer, 1)[1].view(batch.label.size()) == batch.label).sum().item()\n",
    "        n_total += batch.batch_size\n",
    "        train_acc = 100. * n_correct/n_total\n",
    "\n",
    "        loss = criterion(answer, batch.label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # evaluate performance on validation set periodically\n",
    "        if iterations % 20 == 0:\n",
    "            # switch model to evaluation mode\n",
    "            model.eval()\n",
    "            valid_iterator.init_epoch()\n",
    "\n",
    "            # calculate accuracy on validation set\n",
    "            n_val_correct, val_loss = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for val_batch_idx, val_batch in enumerate(valid_iterator):\n",
    "                    answer = model(val_batch)\n",
    "                    n_val_correct += (torch.max(answer, 1)[1].view(val_batch.label.size()) == val_batch.label).sum().item()\n",
    "                    val_loss = criterion(answer, val_batch.label)\n",
    "            val_acc = 100. * n_val_correct / len(val)\n",
    "\n",
    "            print(log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iterator),\n",
    "                100. * (1+batch_idx) / len(train_iterator), loss.item(), val_loss.item(), train_acc, val_acc))\n",
    "            \n",
    "        if iterations % 50 == 0:\n",
    "            \n",
    "            # print progress message\n",
    "            print(val_log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iterator),\n",
    "                100. * (1+batch_idx) / len(train_iterator), loss.item(), ' '*8, n_correct/n_total*100, ' '*12))\n",
    "            \n",
    "            acc_loss.append(loss.item(), n_correct/n_total*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, params.outpath)\n",
    "\n",
    "with open(params.outputs, \"w\") as output:\n",
    "    output.write(str(acc_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
