{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as O\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, vocab, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self):\n",
    "        # gpu\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # word vectors\n",
    "        self.embed_size = 50\n",
    "        self.word_vectors = True\n",
    "        self.glove_path = '/home/ndg/users/jkurre/mnli/utils/embeddings/glove.6B.50d.txt'\n",
    "        # model configs\n",
    "        self.hidden_size = 1024\n",
    "        self.batch_size = 32\n",
    "        self.input_size = 76790\n",
    "        self.output_size = 4\n",
    "        self.n_layers = 2\n",
    "        self.n_cells = 4\n",
    "        self.dropout = 0.5\n",
    "        # training\n",
    "        self.epochs = 5\n",
    "        self.learning_rate = 0.0001\n",
    "        self.outpath = '/home/ndg/users/jkurre/mnli/models/bilstm_revised_onehot.pt' # toggle _onehot.pt\n",
    "\n",
    "params = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data.Field(\n",
    "    lower=True,\n",
    "    tokenize='spacy'\n",
    ")\n",
    "\n",
    "answers = data.Field(\n",
    "    sequential=False\n",
    ")\n",
    "\n",
    "train, val, test = datasets.MultiNLI.splits(\n",
    "    text_field=inputs,\n",
    "    label_field=answers\n",
    "    )\n",
    "\n",
    "inputs.build_vocab(train, val, test)\n",
    "\n",
    "if params.word_vectors:\n",
    "    inputs.vocab.load_vectors(vocab.Vectors(params.glove_path, cache=\".\"))\n",
    "\n",
    "answers.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in inputs vocabulary: 76790\n",
      "Unique tokens in answers vocabulary: 4\n"
     ]
    }
   ],
   "source": [
    "params.n_embed = len(inputs.vocab)\n",
    "params.d_out = len(answers.vocab)\n",
    "\n",
    "print(f\"Unique tokens in inputs vocabulary: {params.n_embed}\")\n",
    "print(f\"Unique tokens in answers vocabulary: {params.d_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=params.batch_size, device=params.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNLIModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embed_size, \n",
    "                 hidden_size, dropout, n_layers, n_cells):\n",
    "        \n",
    "        super(MultiNLIModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_cells = n_cells\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size,\n",
    "                            num_layers=n_layers, dropout=dropout, \n",
    "                            bidirectional=True)\n",
    "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n",
    "        self.fc_output = nn.Linear(hidden_size,  output_size, bias=False)\n",
    "    \n",
    "    def encode(self, pair_embed, batch_size):\n",
    "        state_shape = self.n_cells, batch_size, self.hidden_size\n",
    "        h0 = c0 = pair_embed.new_zeros(state_shape)\n",
    "        outputs, (ht, ct) = self.lstm(pair_embed, (h0, c0))\n",
    "        return ht[-2:].transpose(0, 1).contiguous().view(batch_size, -1)\n",
    "        \n",
    "    def forward(self, pair):\n",
    "        # get batch size\n",
    "        batch_size = pair.batch_size\n",
    "        \n",
    "        # seq_length, batch_size, embed_size\n",
    "        prem_embed = self.embed(pair.premise)\n",
    "        hypo_embed = self.embed(pair.hypothesis)\n",
    "        \n",
    "        # fix word embeddings\n",
    "        prem_embed.detach()\n",
    "        hypo_embed.detach()\n",
    "        \n",
    "        # seq_length * 2, batch_size, embed_size\n",
    "        pair_embed = torch.cat((prem_embed, hypo_embed),0)\n",
    "        pair_embed = self.encode(pair_embed, batch_size)\n",
    "\n",
    "        # seq_length * 2, batch_size, output_size\n",
    "        pair_embed = self.relu(self.fc_hidden(pair_embed))\n",
    "        pair_embed = self.relu(self.fc_output(pair_embed))\n",
    "        \n",
    "        return pair_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiNLIModel(params.input_size, params.output_size, params.embed_size,\n",
    "                      params.hidden_size, params.dropout, params.n_layers, params.n_cells).to(params.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    80     0       500   500/12272       4% 1.326310               33.4188             \n",
      "   163     0      1000  1000/12272       8% 1.203397               34.1281             \n",
      "   244     0      1500  1500/12272      12% 1.338531               34.3896             \n",
      "   330     0      2000  2000/12272      16% 1.264259 1.3567132949829102      32.0203 20.285277636271015\n",
      "   330     0      2000  2000/12272      16% 1.264259               32.0203             \n",
      "   411     0      2500  2500/12272      20% 1.414378               30.0913             \n",
      "   492     0      3000  3000/12272      24% 1.376263               30.6073             \n",
      "   572     0      3500  3500/12272      29% 1.271920               31.2009             \n",
      "   660     0      4000  4000/12272      33% 1.358484 1.3401602506637573      31.4688 34.17218543046358\n",
      "   660     0      4000  4000/12272      33% 1.358484               31.4688             \n",
      "   740     0      4500  4500/12272      37% 1.124092               31.8271             \n",
      "   822     0      5000  5000/12272      41% 1.118876               32.1444             \n",
      "   902     0      5500  5500/12272      45% 1.354874               32.4006             \n",
      "   990     0      6000  6000/12272      49% 1.242898 1.3358784914016724      32.5995 31.920529801324502\n",
      "   990     0      6000  6000/12272      49% 1.242898               32.5995             \n",
      "  1071     0      6500  6500/12272      53% 1.300200               32.7976             \n",
      "  1150     0      7000  7000/12272      57% 1.356145               32.8732             \n",
      "  1233     0      7500  7500/12272      61% 1.368397               32.9992             \n",
      "  1322     0      8000  8000/12272      65% 1.369460 1.4073050022125244      33.1602 36.495160468670406\n",
      "  1322     0      8000  8000/12272      65% 1.369460               33.1602             \n",
      "  1404     0      8500  8500/12272      69% 1.421706               33.2765             \n",
      "  1485     0      9000  9000/12272      73% 1.228620               33.3729             \n",
      "  1565     0      9500  9500/12272      77% 1.306446               33.4405             \n",
      "  1653     0     10000 10000/12272      81% 1.341782 1.3717018365859985      33.5206 32.980132450331126\n",
      "  1653     0     10000 10000/12272      81% 1.341782               33.5206             \n",
      "  1733     0     10500 10500/12272      86% 0.919977               33.8083             \n",
      "  1813     0     11000 11000/12272      90% 0.937831               34.3190             \n",
      "  1892     0     11500 11500/12272      94% 0.998215               34.7182             \n",
      "  1982     0     12000 12000/12272      98% 1.046465 1.1009925603866577      35.1315 38.90983188996434\n",
      "  1982     0     12000 12000/12272      98% 1.046465               35.1315             \n",
      "  2065     1     12500   228/12272       2% 0.974790               45.3947             \n",
      "  2145     1     13000   728/12272       6% 0.944514               45.9306             \n",
      "  2227     1     13500  1228/12272      10% 0.981543               45.7171             \n",
      "  2317     1     14000  1728/12272      14% 0.955503 1.1135470867156982      45.6073 40.28527763627101\n",
      "  2317     1     14000  1728/12272      14% 0.955503               45.6073             \n",
      "  2400     1     14500  2228/12272      18% 1.060206               45.5061             \n",
      "  2480     1     15000  2728/12272      22% 1.071727               45.5668             \n",
      "  2561     1     15500  3228/12272      26% 0.953314               45.7094             \n",
      "  2647     1     16000  3728/12272      30% 1.143596 1.10183846950531      45.9353 40.32603158430973\n",
      "  2647     1     16000  3728/12272      30% 1.143596               45.9353             \n",
      "  2728     1     16500  4228/12272      34% 0.947605               46.0649             \n",
      "  2810     1     17000  4728/12272      39% 1.029096               46.1737             \n",
      "  2893     1     17500  5228/12272      43% 0.966869               46.3502             \n",
      "  2980     1     18000  5728/12272      47% 1.066821 1.140534520149231      46.5215 39.24605196128375\n",
      "  2980     1     18000  5728/12272      47% 1.066821               46.5215             \n",
      "  3063     1     18500  6228/12272      51% 1.077530               46.7004             \n",
      "  3143     1     19000  6728/12272      55% 1.185086               46.8327             \n",
      "  3225     1     19500  7228/12272      59% 1.007451               46.9878             \n",
      "  3315     1     20000  7728/12272      63% 1.057513 1.100732445716858      47.1059 40.8762098828324\n",
      "  3315     1     20000  7728/12272      63% 1.057513               47.1059             \n",
      "  3395     1     20500  8228/12272      67% 0.909893               47.2464             \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = O.Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "log_template =  ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\n",
    "\n",
    "iterations = 0\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(params.epochs):\n",
    "    train_iterator.init_epoch()\n",
    "    n_correct, n_total = 0, 0\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        \n",
    "        # switch model to training mode, clear gradient accumulators\n",
    "        model.train();\n",
    "        opt.zero_grad()\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "        # forward pass\n",
    "        answer = model(batch)\n",
    "        \n",
    "        # calculate accuracy of predictions in the current batch\n",
    "        n_correct += (torch.max(answer, 1)[1].view(batch.label.size()) == batch.label).sum().item()\n",
    "        n_total += batch.batch_size\n",
    "        train_acc = 100. * n_correct/n_total\n",
    "\n",
    "        loss = criterion(answer, batch.label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # evaluate performance on validation set periodically\n",
    "        if iterations % 2000 == 0:\n",
    "            # switch model to evaluation mode\n",
    "            model.eval()\n",
    "            valid_iterator.init_epoch()\n",
    "\n",
    "            # calculate accuracy on validation set\n",
    "            n_val_correct, val_loss = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for val_batch_idx, val_batch in enumerate(valid_iterator):\n",
    "                    answer = model(val_batch)\n",
    "                    n_val_correct += (torch.max(answer, 1)[1].view(val_batch.label.size()) == val_batch.label).sum().item()\n",
    "                    val_loss = criterion(answer, val_batch.label)\n",
    "            val_acc = 100. * n_val_correct / len(val)\n",
    "\n",
    "            print(log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iterator),\n",
    "                100. * (1+batch_idx) / len(train_iterator), loss.item(), val_loss.item(), train_acc, val_acc))\n",
    "        \n",
    "        if iterations % 500 == 0:\n",
    "\n",
    "            # print progress message\n",
    "            print(log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iterator),\n",
    "                100. * (1+batch_idx) / len(train_iterator), loss.item(), ' '*8, n_correct/n_total*100, ' '*12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, params.outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
