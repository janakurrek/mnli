{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as O\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, vocab, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self):\n",
    "        # gpu\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # word vectors\n",
    "        self.embed_size = 50\n",
    "        self.word_vectors = True\n",
    "        self.glove_path = '/home/ndg/users/jkurre/mnli/utils/embeddings/glove.6B.50d.txt'\n",
    "        # model configs\n",
    "        self.hidden_size = 1024\n",
    "        self.batch_size = 32\n",
    "        self.input_size = 76790\n",
    "        self.output_size = 4\n",
    "        self.n_layers = 2\n",
    "        self.n_cells = 4\n",
    "        self.dropout = 0.5\n",
    "        # training\n",
    "        self.epochs = 5\n",
    "        self.learning_rate = 0.0001\n",
    "        self.outpath = '/home/ndg/users/jkurre/mnli/models/bilstm_revised.pt' # _onehot.pt\n",
    "\n",
    "params = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data.Field(\n",
    "    lower=True,\n",
    "    tokenize='spacy'\n",
    ")\n",
    "\n",
    "answers = data.Field(\n",
    "    sequential=False\n",
    ")\n",
    "\n",
    "train, val, test = datasets.MultiNLI.splits(\n",
    "    text_field=inputs,\n",
    "    label_field=answers\n",
    "    )\n",
    "\n",
    "inputs.build_vocab(train, val, test)\n",
    "\n",
    "if params.word_vectors:\n",
    "    inputs.vocab.load_vectors(vocab.Vectors(params.glove_path, cache=\".\"))\n",
    "\n",
    "answers.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in inputs vocabulary: 76790\n",
      "Unique tokens in answers vocabulary: 4\n"
     ]
    }
   ],
   "source": [
    "params.n_embed = len(inputs.vocab)\n",
    "params.d_out = len(answers.vocab)\n",
    "\n",
    "print(f\"Unique tokens in inputs vocabulary: {params.n_embed}\")\n",
    "print(f\"Unique tokens in answers vocabulary: {params.d_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=params.batch_size, device=params.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNLIModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embed_size, device,\n",
    "                 hidden_size, batch_size, dropout, n_layers, n_cells):\n",
    "        \n",
    "        super(MultiNLIModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_cells = n_cells\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size,\n",
    "                            num_layers=n_layers, dropout=dropout, \n",
    "                            bidirectional=True)\n",
    "        \"\"\"self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size, bias=False)\"\"\"\n",
    "        \"\"\"self.fc_output = nn.Linear(hidden_size,  output_size, bias=False)\"\"\"\n",
    "        self.fc_hidden1 = nn.Linear(hidden_size * 2, batch_size, bias=False)\n",
    "        self.fc_hidden2 = nn.Linear(batch_size, hidden_size * 2, bias=False)\n",
    "        self.fc_hidden3 = nn.Linear(hidden_size * 2,  output_size, bias=False)\n",
    "    \n",
    "    def encode(self, embed):\n",
    "        state_shape = self.n_cells, self.batch_size, self.hidden_size\n",
    "        h0 = c0 = embed.new_zeros(state_shape)\n",
    "        outputs, (ht, ct) = self.lstm(embed, (h0, c0))\n",
    "        return ht[-2:].transpose(0, 1).contiguous().view(self.batch_size, -1)\n",
    "    \n",
    "    \"\"\"\n",
    "    def forward(self, pair):\n",
    "        # get batch size\n",
    "        batch_size = pair.batch_size\n",
    "        \n",
    "        # seq_length, batch_size, embed_size\n",
    "        prem_embed = self.embed(pair.premise)\n",
    "        hypo_embed = self.embed(pair.hypothesis)\n",
    "        \n",
    "        # fix word embeddings\n",
    "        prem_embed.detach()\n",
    "        hypo_embed.detach()\n",
    "        \n",
    "        # seq_length * 2, batch_size, embed_size\n",
    "        pair_embed = torch.cat((prem_embed, hypo_embed),0)\n",
    "        pair_embed = self.encode(pair_embed, batch_size)\n",
    "\n",
    "        # seq_length * 2, batch_size, output_size\n",
    "        pair_embed = self.relu(self.fc_hidden(pair_embed))\n",
    "        pair_embed = self.relu(self.fc_output(pair_embed))\n",
    "        \n",
    "        return pair_embed\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, pair):\n",
    "        \n",
    "        # conditionally update batch size in linear layers\n",
    "        if pair.batch_size != self.batch_size:\n",
    "            self.batch_size = pair.batch_size\n",
    "            self.fc_hidden1 = nn.Linear(self.hidden_size * 2, pair.batch_size, bias=False).to(self.device)\n",
    "            self.fc_hidden2 = nn.Linear(pair.batch_size, self.hidden_size * 2, bias=False).to(self.device)\n",
    "\n",
    "        # seq_length, batch_size, embed_size\n",
    "        prem_embed = self.embed(pair.premise)\n",
    "        hypo_embed = self.embed(pair.hypothesis)\n",
    "        \n",
    "        # fix word embeddings\n",
    "        prem_embed.detach()\n",
    "        hypo_embed.detach()\n",
    "        \n",
    "        # batch_size, hidden_size * 2\n",
    "        prem_embed = self.encode(prem_embed)\n",
    "        hypo_embed = self.encode(hypo_embed)\n",
    "        \n",
    "        # batch_size, batch_size\n",
    "        prem_embed = self.relu(self.fc_hidden1(prem_embed))\n",
    "        hypo_embed = self.relu(self.fc_hidden1(hypo_embed))\n",
    "        \n",
    "        # batch_size, hidden_size * 2\n",
    "        pair_embed = prem_embed - hypo_embed\n",
    "        pair_embed = self.relu(self.fc_hidden2(pair_embed))\n",
    "        \n",
    "        # hidden_size * 2, output_size\n",
    "        pair_output = self.relu(self.fc_hidden3(pair_embed))\n",
    "        \n",
    "        return pair_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiNLIModel(params.input_size, params.output_size, params.embed_size, params.device,\n",
    "                      params.hidden_size, params.batch_size, params.dropout, params.n_layers, params.n_cells).to(params.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0f6a07369cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ndg/users/jkurre/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = O.Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "val_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{}'.split(','))\n",
    "log_template =  ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\n",
    "\n",
    "iterations = 0\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(params.epochs):\n",
    "    train_iterator.init_epoch()\n",
    "    n_correct, n_total = 0, 0\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        \n",
    "        # switch model to training mode, clear gradient accumulators\n",
    "        model.train();\n",
    "        opt.zero_grad()\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "        # forward pass\n",
    "        answer = model(batch)\n",
    "        \n",
    "        # calculate accuracy of predictions in the current batch\n",
    "        n_correct += (torch.max(answer, 1)[1].view(batch.label.size()) == batch.label).sum().item()\n",
    "        n_total += batch.batch_size\n",
    "        train_acc = 100. * n_correct/n_total\n",
    "\n",
    "        loss = criterion(answer, batch.label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # evaluate performance on validation set periodically\n",
    "        if iterations % 2000 == 0:\n",
    "            # switch model to evaluation mode\n",
    "            model.eval()\n",
    "            valid_iterator.init_epoch()\n",
    "\n",
    "            # calculate accuracy on validation set\n",
    "            n_val_correct, val_loss = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for val_batch_idx, val_batch in enumerate(valid_iterator):\n",
    "                    answer = model(val_batch)\n",
    "                    n_val_correct += (torch.max(answer, 1)[1].view(val_batch.label.size()) == val_batch.label).sum().item()\n",
    "                    val_loss = criterion(answer, val_batch.label)\n",
    "            val_acc = 100. * n_val_correct / len(val)\n",
    "\n",
    "            print(log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iterator),\n",
    "                100. * (1+batch_idx) / len(train_iterator), loss.item(), val_loss.item(), train_acc, val_acc))\n",
    "        \n",
    "        if iterations % 500 == 0:\n",
    "            \n",
    "            # print progress message\n",
    "            print(val_log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iterator),\n",
    "                100. * (1+batch_idx) / len(train_iterator), loss.item(), ' '*8, n_correct/n_total*100, ' '*12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, params.outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.attention = nn.Linear(params.hidden_size, params.hidden_size)\n",
    "    \n",
    "    def forward(self, hidden, enc_outputs)\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.n_cells = params.n_cells\n",
    "        self.batch_size = params.batch_size\n",
    "        self.hidden_size = params.hidden_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(params.dropout)\n",
    "        self.embed = self.embed = nn.Embedding(params.input_size, params.embed_size)\n",
    "        self.lstm = nn.LSTM(params.embed_size, params.hidden_size,\n",
    "                            num_layers=params.n_layers, dropout=params.dropout, \n",
    "                            bidirectional=True)\n",
    "    def forward(pair):\n",
    "        # [seq_length, batch_size, embed_size]\n",
    "        prem_embed = self.dropout(self.embed(pair.premise))\n",
    "        hypo_embed = self.dropout(self.embed(pair.hypothesis))\n",
    "        \n",
    "        # fix word embeddings\n",
    "        prem_embed.detach()\n",
    "        hypo_embed.detach()\n",
    "        \n",
    "        # pass through bidirectional lstm\n",
    "        state_shape = self.n_cells, self.batch_size, self.hidden_size\n",
    "        h0 = c0 = embed.new_zeros(state_shape)\n",
    "        outputs, (ht, ct) = self.lstm(embed, (h0, c0))\n",
    "        outputs = ht[-2:].transpose(0, 1).contiguous().view(self.batch_size, -1)\n",
    "        print(outputs.shape)\n",
    "        return outputs\n",
    "    \n",
    "class MultiNLIAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def forward(self, pair):\n",
    "\n",
    "        # batch_size, hidden_size * 2\n",
    "        outputs = self.encoder(pair)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
